{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Spark and PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Author: Ra Inta, written for BH Analytics*\n",
    "\n",
    "*Copyright 2017-2019, BH Analytics, LLC*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The purpose of this lecture is to gain some grounding in the Spark environment, and obtain conceptual understanding of distributed computing models in general. This will be performed via the Spark API to Python, PySpark.  \n",
    "   \n",
    "In particular, we will cover:\n",
    " *  Conceptual models of Spark and the RDD distributed compute model\n",
    " *  Initiating Spark instances on a cluster\n",
    " *  Transformations, actions and where Spark performs lazy evaluation\n",
    " *  How to create, query and manipulate Spark SQL DataFrames\n",
    " *  How to persist Spark objects to improve performance\n",
    " *  Type conversion and filtering of Spark SQL DataFrames\n",
    " *  Exploratory Data Analysis (EDA) on large datasets using Spark\n",
    " *  User-defined functions for customized work-flows\n",
    " *  How to perform machine learning within Spark\n",
    "\n",
    "Spark is a popular framework for good reason, being one of the most widely applied Big Data frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask\n",
    "import pyspark\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark\n",
    "\n",
    "PySpark gives us an interface into using a Spark clustered computing environment from Python.\n",
    "\n",
    "For the demonstrations below we will use a local Spark installation, i.e. installed on a single machine. The scope of installing Spark is beyond this tutorial, since its normal use case will be on a distributed cluster. Here, we want to focus on core syntax of interacting with Spark data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Organization\n",
    "\n",
    "The Spark API is organized around several sub-packages and classes. The main ones we will mention are:\n",
    "\n",
    "- `SparkContext` and `SparkSession`\n",
    "- RDD / DataFrame - our data containers\n",
    "- `pyspark.sql` - for querying Spark data sources\n",
    "- `pyspark.ml` and `pyspark.mllib` - for machine learning on Spark sources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark Up and Running\n",
    "\n",
    "Once PySpark is installed, calling PySpark from a terminal or shell results in output like the following:\n",
    "\n",
    "```\n",
    "Python 3.6.8 |Anaconda custom (64-bit)| (default, Dec 29 2018, 19:04:46)\n",
    "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)] on darwin\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    "19/04/28 16:29:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform...using builtin-java classes where applicable\n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.0\n",
    "      /_/\n",
    "\n",
    "Using Python version 3.6.8 (default, Dec 29 2018 19:04:46)\n",
    "SparkSession available as 'spark'.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Session and Context\n",
    "\n",
    "When we spin up a session of a Spark cluster, we generate a sparkSession object that maintains information about our connection with the cluster.\n",
    "\n",
    "A Spark application contains several components that govern how the cluster accomplishes its jobs. These components are the *Driver,* the *Master,* the *Cluster Manager,* and the *Executors(s)* which run on *Worker* nodes. All of these run inside of a Java virtual machine (JVM.) \n",
    "\n",
    "The *Driver* creates a sparkSession object, which itself includes the sparkContext object, as well as the SQLContext and HiveContext objects.\n",
    "\n",
    "The sparkContext object also represents a connection to the remote or local spark cluster, and was the main entry point for earlier versions of Spark. \n",
    "\n",
    "If we ask for the sparkContext object (as below), we get a basic report about the session, including a link to a web app interface displaying up-to-date resource allocation (`Spark UI`, usually at port 4040)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('demo').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>demo</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=demo>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the URL for the `Spark UI` resource monitor.\n",
    "\n",
    "At the end of every Spark Session, we should terminate the Spark engine (although don't do it yet!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sc.stop()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Driver\n",
    "\n",
    "The role of the *Driver* is to plan the application. It translates **transformations** (operations that manipulate data) and **actions** (operations that request output) and convert these into **graphs** (technically, \"Directed Acyclic Graphs\"; DAGs) that represent steps of the computation. These are conceptually similar to the *task graphs* of **dask.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data into a Spark environment\n",
    "\n",
    "Let's load some data into the Spark environment and examine it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ra/host/BH_Analytics/manufacturing/notebooks_drafts'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a `README.txt` for some CNC tooling data we'll use later. Let's open it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnc_readme = spark.read.text('data/manufacturing/CNC_Tool_Wear/README.txt')\n",
    "cnc_readme.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(value='CNC MILLING DATASET - UNIVERSITY OF MICHIGAN SMART LAB')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnc_readme.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='CNC MILLING DATASET - UNIVERSITY OF MICHIGAN SMART LAB'),\n",
       " Row(value=''),\n",
       " Row(value='April 2018')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnc_readme.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['value']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnc_readme.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNC MILLING DATASET - UNIVERSITY OF MICHIGAN S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>April 2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A series of machining experiments were run on ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               value\n",
       "0  CNC MILLING DATASET - UNIVERSITY OF MICHIGAN S...\n",
       "1                                                   \n",
       "2                                         April 2018\n",
       "3                                                   \n",
       "4  A series of machining experiments were run on ..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnc_readme.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cnc_readme)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD Resilient Distributed Dataset\n",
    "\n",
    "Here, we've created a **Resilient Distributed Dataset (RDD)** from a text file. An RDD is the core data object in Spark. \n",
    "\n",
    "- Resilient - the dataset is robust to losses of nodes\n",
    "- Distributed - divided into one or several partitions and distributed as in-memory objects across Worker nodes. \n",
    "- Dataset - consists of records.\n",
    "\n",
    "Furthermore, RDDs are:\n",
    "\n",
    "- immutable. They cannot be updated, only transformed into new RDDs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames\n",
    "\n",
    "From Spark 2.0, we can interact with Spark data objects as DataFrames, which gives a familiarity to those coming to Spark from pandas and other analytic workflows. We'll concentrate on the modern DataFrame syntax in this notebook.\n",
    "\n",
    "Functionally, a Spark SQL DataFrame is similar to a pandas DataFrame (which it is closely modelled after). You can easily create a Spark DataFrame from a pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(A=0.0, B=1.0, C=2.0, D=3.0, E=4.0),\n",
       " Row(A=1.0, B=2.0, C=3.0, D=4.0, E=5.0),\n",
       " Row(A=2.0, B=3.0, C=4.0, D=5.0, E=6.0),\n",
       " Row(A=3.0, B=4.0, C=5.0, D=6.0, E=7.0),\n",
       " Row(A=4.0, B=5.0, C=6.0, D=7.0, E=8.0)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['A', 'B', 'C', 'D', 'E']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_cols = 5\n",
    "n_rows = 10\n",
    "\n",
    "pandas_df = pd.DataFrame(\n",
    "    {chr(x + 65): range(x, x + n_rows) for x in range(n_cols)},  \n",
    "                   dtype=np.float64) \n",
    "\n",
    "spark_df = spark.createDataFrame(pandas_df)\n",
    "\n",
    "spark_df.head(5)\n",
    "spark_df.columns\n",
    "\n",
    "type(spark_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, generation of Spark SQL DataFrames are not as flexible as their pandas counterparts, partly because you have to specify the schema in advance (which was inferred, in the case of importing the pandas DataFrame above). This strictness is a result of the distributed nature of Spark objects in general.\n",
    "\n",
    "The schema is specified as a list argument in `.createDataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(A=0, B=1, C=2, D=3, E=4),\n",
       " Row(A=1, B=2, C=3, D=4, E=5),\n",
       " Row(A=2, B=3, C=4, D=5, E=6)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_list = list( tuple( range(x, x + n_cols) ) for x in range(n_rows) )\n",
    "schema_list = [chr(x+65) for x in range(n_cols)]  # Lazy way of writing out A-E\n",
    "spark_df2 = spark.createDataFrame(num_list, schema_list) \n",
    "\n",
    "spark_df2.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other ways to create DataFrames\n",
    "\n",
    "We can create DataFrames via several methods and sources, for instance: `wholeTextFiles`, `jdbc`, and `json.` Here, we'll read data in from a `CSV` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnc_data = spark.read.csv('../data/manufacturing/CNC_Tool_Wear/cnc_experiments_all.csv', \n",
    "                          header=True, inferSchema=True)\n",
    "type(cnc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save this DataFrame to Parquet format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnc_data.write.parquet('../data/cnc_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnc_data_dta = spark.read.parquet(\"../data/cnc_data.parquet\")\n",
    "type(cnc_data_dta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the DataFrame is distributed, obtaining its shape may be an expensive operation, compared to its pandas analog:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_rows: 25286, n_col: 54\n"
     ]
    }
   ],
   "source": [
    "print(\"n_rows: {}, n_col: {}\".format(cnc_data.count(), len(cnc_data.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the use of `.count` to get the rows, which are likely to be distributed, while the `.columns` are not.\n",
    "\n",
    "\n",
    "There are a lot of columns. Let's select a few key ones, using the `.select` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = cnc_data.columns[0:2] + cnc_data.columns[:-4:-1]\n",
    "cnc_data_sub = cnc_data.select(col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then query the schema using `.printSchema`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- X1_ActualPosition: double (nullable = true)\n",
      " |-- X1_ActualVelocity: double (nullable = true)\n",
      " |-- passed_visual_inspection: string (nullable = true)\n",
      " |-- machining_finalized: string (nullable = true)\n",
      " |-- tool_condition: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnc_data_sub.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations, Actions, Lazy Evaluation\n",
    "\n",
    "Transformations (such as the `.select` above) are _evaluated lazily_. Namely:\n",
    "\n",
    "- Transformations perform operations against an RDD/DataFrame and result in a corresponding new object. `map,` `filter` and `select` are common transformations.\n",
    "\n",
    "- Actions return values or data. `reduce,` `collect,` `count,` `show,` and `save` are common actions.\n",
    "\n",
    "- Lazy evaluation is the principle of deferring processing until an action is called. \n",
    "\n",
    "In other words, execution is only performed when necessary, such as triggered by an action, such as `.show`. However, such actions are required to gather data from the distributed nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+------------------------+-------------------+--------------+\n",
      "|summary| X1_ActualPosition|   X1_ActualVelocity|passed_visual_inspection|machining_finalized|tool_condition|\n",
      "+-------+------------------+--------------------+------------------------+-------------------+--------------+\n",
      "|  count|             25286|               25286|                   23125|              25286|         25286|\n",
      "|   mean|159.05204460966542|-0.28865716206596675|                    null|               null|          null|\n",
      "| stddev|19.330872672882187|   5.658260347957942|                    null|               null|          null|\n",
      "|    min|             141.0|               -20.4|                      no|                 no|        unworn|\n",
      "|    25%|             145.0|               -2.05|                    null|               null|          null|\n",
      "|    50%|             153.0|                 0.0|                    null|               null|          null|\n",
      "|    75%|             162.0|                 0.2|                    null|               null|          null|\n",
      "|    max|             198.0|                50.7|                     yes|                yes|          worn|\n",
      "+-------+------------------+--------------------+------------------------+-------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnc_data_sub.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform transformations between columns using `.withColumn` (here we use the physical relationship $v = \\frac{\\Delta x}{\\Delta t}$): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+------------------------+-------------------+--------------+-------------------+\n",
      "|X1_ActualPosition|X1_ActualVelocity|passed_visual_inspection|machining_finalized|tool_condition|           duration|\n",
      "+-----------------+-----------------+------------------------+-------------------+--------------+-------------------+\n",
      "|            198.0|              0.0|                     yes|                yes|        unworn|               null|\n",
      "|            198.0|            -10.8|                     yes|                yes|        unworn|-18.333333333333332|\n",
      "|            196.0|            -17.8|                     yes|                yes|        unworn| -11.01123595505618|\n",
      "|            194.0|            -18.0|                     yes|                yes|        unworn|-10.777777777777779|\n",
      "|            193.0|            -17.9|                     yes|                yes|        unworn|-10.782122905027935|\n",
      "+-----------------+-----------------+------------------------+-------------------+--------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnc_data_sub\\\n",
    ".withColumn(\"duration\", cnc_data_sub[\"X1_ActualPosition\"]/cnc_data_sub[\"X1_ActualVelocity\"])\\\n",
    ".show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persistence\n",
    "\n",
    "RDDs and DataFrames exist primarily in memory on Executors. They are temporary objects that exist only when needed, when they are transformed into new objects, the old are removed permanently. \n",
    "\n",
    "If an RDD/DataFrame is used for more than one transformation, this might be costly (since it has to be evaluated from scratch each time.) In that case, the object can be `persisted.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[X1_ActualPosition: double, X1_ActualVelocity: double, passed_visual_inspection: string, machining_finalized: string, tool_condition: string]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnc_data_sub.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `.persist` you can specify the storage level (disk vs memory). The default is a judicious combination (`MEMORY_AND_DISK`). If the RDD doesn't fit in memory, store the partitions that don't fit disk, and read them as needed.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformations\n",
    "\n",
    "Core *transformations* include\n",
    "\n",
    "- map - evaluates a function for each element (like a numpy uFunc)\n",
    "- flatmap - evaluates a function, but removes a level of nesting\n",
    "- filter - evaluates a boolean comparison against each element\n",
    "- distinct - returns a new RDD with unique elements\n",
    "- select - returns a sub-set of columns \n",
    "- groupBy and sortBy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions\n",
    "\n",
    "Core *actions* include\n",
    "\n",
    "- count - counts elements\n",
    "- collect - returns elements in the RDD to the Driver\n",
    "- take - returns the first n elements (not necessarily ordered)\n",
    "- top - returns the top n elements (ordered and returned in descending order)\n",
    "- first\n",
    "- reduce\n",
    "- fold\n",
    "- foreach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data management\n",
    "\n",
    "We can query the data types by examining the schema of the RDD/DataFrame, with `.printSchema` as we did above: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- X1_ActualPosition: double (nullable = true)\n",
      " |-- X1_ActualVelocity: double (nullable = true)\n",
      " |-- passed_visual_inspection: string (nullable = true)\n",
      " |-- machining_finalized: string (nullable = true)\n",
      " |-- tool_condition: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnc_data_sub.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also specifically examine the data types with `.dtypes`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('X1_ActualPosition', 'double'),\n",
       " ('X1_ActualVelocity', 'double'),\n",
       " ('passed_visual_inspection', 'string'),\n",
       " ('machining_finalized', 'string'),\n",
       " ('tool_condition', 'string')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnc_data_sub.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting column types\n",
    "\n",
    "To convert a column type, we can use `cast.`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<b'CAST(X1_ActualVelocity AS INT)'>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('X1_ActualPosition', 'double'),\n",
       " ('X1_ActualVelocity', 'double'),\n",
       " ('passed_visual_inspection', 'string'),\n",
       " ('machining_finalized', 'string'),\n",
       " ('tool_condition', 'string')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnc_data_sub[\"X1_ActualVelocity\"].cast(\"int\")\n",
    "cnc_data_sub.dtypes  # still the original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However recall that RDDs are immutable. Hence, if we wish to transform only one column, we need to join that new column back to the original and return a new RDD. `withColumn` will return a new data frame back with an added column, in this case a transformed column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+------------------------+-------------------+--------------+----------------+\n",
      "|X1_ActualPosition|X1_ActualVelocity|passed_visual_inspection|machining_finalized|tool_condition|velocity_10x_int|\n",
      "+-----------------+-----------------+------------------------+-------------------+--------------+----------------+\n",
      "|            198.0|              0.0|                     yes|                yes|        unworn|               0|\n",
      "|            198.0|            -10.8|                     yes|                yes|        unworn|            -100|\n",
      "|            196.0|            -17.8|                     yes|                yes|        unworn|            -170|\n",
      "|            194.0|            -18.0|                     yes|                yes|        unworn|            -180|\n",
      "|            193.0|            -17.9|                     yes|                yes|        unworn|            -170|\n",
      "+-----------------+-----------------+------------------------+-------------------+--------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnc_data_sub2 = cnc_data_sub\\\n",
    "    .withColumn(\"velocity_10x_int\", 10*cnc_data_sub[\"X1_ActualVelocity\"]\\\n",
    "    .cast(\"int\"))\\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using transformations and actions for Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary statistics\n",
    "\n",
    "We can retrieve summary statistics with `describe`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+--------------------+------------------------+-------------------+--------------+\n",
      "|summary| X1_ActualPosition|   X1_ActualVelocity|passed_visual_inspection|machining_finalized|tool_condition|\n",
      "+-------+------------------+--------------------+------------------------+-------------------+--------------+\n",
      "|  count|             25286|               25286|                   23125|              25286|         25286|\n",
      "|   mean|159.05204460966542|-0.28865716206596675|                    null|               null|          null|\n",
      "| stddev|19.330872672882187|   5.658260347957942|                    null|               null|          null|\n",
      "|    min|             141.0|               -20.4|                      no|                 no|        unworn|\n",
      "|    max|             198.0|                50.7|                     yes|                yes|          worn|\n",
      "+-------+------------------+--------------------+------------------------+-------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnc_data_sub.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In a Jupyter notebook, the results look better if converted to a pandas data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>X1_ActualPosition</th>\n",
       "      <th>X1_ActualVelocity</th>\n",
       "      <th>passed_visual_inspection</th>\n",
       "      <th>machining_finalized</th>\n",
       "      <th>tool_condition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>25286</td>\n",
       "      <td>25286</td>\n",
       "      <td>23125</td>\n",
       "      <td>25286</td>\n",
       "      <td>25286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>159.05204460966542</td>\n",
       "      <td>-0.28865716206596675</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>19.330872672882187</td>\n",
       "      <td>5.658260347957942</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>141.0</td>\n",
       "      <td>-20.4</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>unworn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>198.0</td>\n",
       "      <td>50.7</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>worn</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary   X1_ActualPosition     X1_ActualVelocity passed_visual_inspection  \\\n",
       "0   count               25286                 25286                    23125   \n",
       "1    mean  159.05204460966542  -0.28865716206596675                     None   \n",
       "2  stddev  19.330872672882187     5.658260347957942                     None   \n",
       "3     min               141.0                 -20.4                       no   \n",
       "4     max               198.0                  50.7                      yes   \n",
       "\n",
       "  machining_finalized tool_condition  \n",
       "0               25286          25286  \n",
       "1                None           None  \n",
       "2                None           None  \n",
       "3                  no         unworn  \n",
       "4                 yes           worn  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnc_data_sub.describe().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce clutter with `.select`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|X1_ActualPosition|\n",
      "+-----------------+\n",
      "|            198.0|\n",
      "|            198.0|\n",
      "|            196.0|\n",
      "|            194.0|\n",
      "|            193.0|\n",
      "+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnc_data_sub.select(\"X1_ActualPosition\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many distinct values of CNC mill head X1 position?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnc_data_sub.select(\"X1_ActualPosition\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|X1_ActualPosition|\n",
      "+-----------------+\n",
      "|            184.0|\n",
      "|            147.0|\n",
      "|            170.0|\n",
      "|            169.0|\n",
      "|            160.0|\n",
      "+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnc_data_sub.select(\"X1_ActualPosition\").distinct().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating a histogram\n",
    "\n",
    "To display a histogram, first we must compute a histogram. Then, when we have the results back, we can visualize it using pandas or seaborn.\n",
    "\n",
    "Computing a histogram (or quantiles) is a non-trivial task on a distributed dataset. But the RDD `histogram()` method takes care of the heavy work for you.\n",
    "\n",
    "There is also the [pyspark_dist_explore package,](https://github.com/Bergvca/pyspark_dist_explore) but it is not part of anaconda.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([6.9800e+02, 1.4000e+02, 1.9900e+02, 1.3100e+02, 4.6190e+03,\n",
       "        1.3712e+04, 4.0180e+03, 1.1120e+03, 7.0000e+01, 2.2800e+02,\n",
       "        5.9000e+01, 1.9400e+02, 1.0000e+00, 4.0000e+00, 3.0000e+00,\n",
       "        0.0000e+00, 5.0000e+00, 2.0000e+00, 5.3000e+01, 3.8000e+01]),\n",
       " array([-20.4  , -16.845, -13.29 ,  -9.735,  -6.18 ,  -2.625,   0.93 ,\n",
       "          4.485,   8.04 ,  11.595,  15.15 ,  18.705,  22.26 ,  25.815,\n",
       "         29.37 ,  32.925,  36.48 ,  40.035,  43.59 ,  47.145,  50.7  ]),\n",
       " <a list of 20 Patch objects>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins, counts = cnc_data_sub.select(\"X1_ActualVelocity\").rdd.flatMap(lambda x: x).histogram(20)\n",
    "plt.hist(bins[:-1], bins=bins, weights=counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[145.0, 153.0, 162.0]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnc_data_sub.approxQuantile(\"X1_ActualPosition\", [0.25, 0.5, 0.75], 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1,\n",
       " 0.2,\n",
       " 0.30000000000000004,\n",
       " 0.4,\n",
       " 0.5,\n",
       " 0.6,\n",
       " 0.7000000000000001,\n",
       " 0.8,\n",
       " 0.9,\n",
       " 1.0]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(0.1, 1.1, 0.1).tolist() # note the floating point error!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[141.0, 143.0, 146.0, 150.0, 153.0, 157.0, 160.0, 162.0, 198.0, 198.0]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnc_data_sub.approxQuantile(\"X1_ActualPosition\", np.arange(0.1, 1.1, 0.1).tolist(), 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group-by analysis\n",
    "\n",
    "We can use `groupBy` syntax to use Split-Apply-Combine techniques on Spark DataFrames, but we should consider some potential issues with performance.\n",
    "\n",
    "Using `groupBy` on a Spark dataframe is similar to pandas `groupby` functionality. We can group on a column or multiple columns, for example, to get categorical variable counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|tool_condition|count|\n",
      "+--------------+-----+\n",
      "|        unworn|11978|\n",
      "|          worn|13308|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnc_data_sub.groupBy(\"tool_condition\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiple group-bys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tool_condition</th>\n",
       "      <th>passed_visual_inspection</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unworn</td>\n",
       "      <td>yes</td>\n",
       "      <td>10984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>worn</td>\n",
       "      <td>no</td>\n",
       "      <td>3942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>unworn</td>\n",
       "      <td>None</td>\n",
       "      <td>994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>worn</td>\n",
       "      <td>yes</td>\n",
       "      <td>8199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>worn</td>\n",
       "      <td>None</td>\n",
       "      <td>1167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tool_condition passed_visual_inspection  count\n",
       "0         unworn                      yes  10984\n",
       "1           worn                       no   3942\n",
       "2         unworn                     None    994\n",
       "3           worn                      yes   8199\n",
       "4           worn                     None   1167"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnc_data_sub.groupBy(\"tool_condition\", \"passed_visual_inspection\").count().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crosstab\n",
    "\n",
    "Unfortunately, unlike in pandas, the `normalize` keyword is not yet implemented in the Spark SQL version of `.crosstab`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------+----+----+-----+\n",
      "|tool_condition_passed_visual_inspection|  no|null|  yes|\n",
      "+---------------------------------------+----+----+-----+\n",
      "|                                   worn|3942|1167| 8199|\n",
      "|                                 unworn|   0| 994|10984|\n",
      "+---------------------------------------+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnc_data_sub.crosstab(\"tool_condition\", \"passed_visual_inspection\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boxplot\n",
    "\n",
    "By virtue of its distributed nature, PySpark doesn't have any plotting functionality. To plot data, you need to bring it out of Spark and into pandas or numpy (_i.e._ wholly in-memory), and then use your usual plot methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f5b455a7860>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "box_df = cnc_data_sub.select([\"X1_ActualPosition\", \"tool_condition\"]).toPandas()\n",
    "box_df.boxplot(by=\"tool_condition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation\n",
    "\n",
    "The Spark SQL `.corr` method easily computes the correlation between any two columns. However, currently only the Pearson correlation coefficient is supported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.02792458000812223"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnc_data_sub.corr(\"X1_ActualPosition\", \"X1_ActualVelocity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|tool_condition|count|\n",
      "+--------------+-----+\n",
      "|        unworn|11978|\n",
      "|          worn|13308|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnc_data_sub.groupBy(\"tool_condition\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------------+-----+\n",
      "|tool_condition|passed_visual_inspection|count|\n",
      "+--------------+------------------------+-----+\n",
      "|        unworn|                     yes|10984|\n",
      "|          worn|                      no| 3942|\n",
      "|        unworn|                    null|  994|\n",
      "|          worn|                     yes| 8199|\n",
      "|          worn|                    null| 1167|\n",
      "+--------------+------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnc_data_sub.groupBy(\"tool_condition\", \"passed_visual_inspection\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "One principle of optimizing Spark is the **Filter Early, Filter Often** principle. Filtering out noisy or unwanted data early can significantly reduce shuffles and have a huge impact on performance.\n",
    "\n",
    "For Dataframes, filtering is the `filter` method.\n",
    "\n",
    "Perform `filter` and `map` operations before any operation that incurs a shuffle, such as a `reduceByKey.` \n",
    "\n",
    "Also, leverage the UI to diagnose and improve shuffle performance. For more details, consult ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19183"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnc_data_sub.filter(cnc_data_sub[\"passed_visual_inspection\"] == \"yes\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1_ActualVelocity</th>\n",
       "      <th>tool_condition</th>\n",
       "      <th>passed_visual_inspection</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2149</th>\n",
       "      <td>14.10</td>\n",
       "      <td>worn</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>5.95</td>\n",
       "      <td>unworn</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6862</th>\n",
       "      <td>3.08</td>\n",
       "      <td>worn</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7711</th>\n",
       "      <td>0.15</td>\n",
       "      <td>worn</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5712</th>\n",
       "      <td>3.00</td>\n",
       "      <td>worn</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      X1_ActualVelocity tool_condition passed_visual_inspection\n",
       "2149              14.10           worn                     None\n",
       "1243               5.95         unworn                      yes\n",
       "6862               3.08           worn                      yes\n",
       "7711               0.15           worn                      yes\n",
       "5712               3.00           worn                      yes"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnc_data_sub\\\n",
    "    .filter(cnc_data_sub[\"X1_ActualVelocity\"] > 0)\\\n",
    "    .select([\"X1_ActualVelocity\", \"tool_condition\", \"passed_visual_inspection\"])\\\n",
    "    .toPandas()\\\n",
    "    .sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL\n",
    "\n",
    "The SQL abstractions in Spark are based on the `Hive` project, which is a layer of abstraction on top of Hadoop MapReduce. `HiveQL` was parsed by a Hive client and mapped to MapReduce operations. \n",
    "\n",
    "Since then, `Spark SQL` was introduced, which brings a HiveQL-compatible extension to RDD-based storage.\n",
    "\n",
    "One of the main components of Spark SQL is the DataFrame API which you've already been using in this module! Spark SQL includes native support for Parquet files.\n",
    "\n",
    "The `sparkSession` is the main entry point for Spark SQL applications, as we've already seen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UDFs\n",
    "\n",
    "It's straight-forward -- perhaps often necessary -- to create User-Defined Functions within the Spark SQL DataFrame API as well. We do need to be conscious of the returnType of a UDF and declare that explicitly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|   X1_ActualVelocity|\n",
      "+-------+--------------------+\n",
      "|  count|               25286|\n",
      "|   mean|-0.28865716206596675|\n",
      "| stddev|   5.658260347957942|\n",
      "|    min|               -20.4|\n",
      "|    max|                50.7|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnc_data_sub.select(\"X1_ActualVelocity\").describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType \n",
    "\n",
    "getSign = udf(lambda x: '-' if x < 0 else '+', StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+\n",
      "|X1_ActualVelocity|v_sign|\n",
      "+-----------------+------+\n",
      "|              0.0|     +|\n",
      "|            -10.8|     -|\n",
      "|            -17.8|     -|\n",
      "|            -18.0|     -|\n",
      "|            -17.9|     -|\n",
      "|            -17.6|     -|\n",
      "|            -17.9|     -|\n",
      "|            -17.8|     -|\n",
      "|            -17.9|     -|\n",
      "|            -17.9|     -|\n",
      "+-----------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnc_data_sub\\\n",
    "    .select(cnc_data_sub[\"X1_ActualVelocity\"], getSign(cnc_data_sub[\"X1_ActualVelocity\"])\\\n",
    "    .alias(\"v_sign\"))\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning with Spark ML: Data Preparation\n",
    "\n",
    "As is common with machine learning libraries, a lot of data preparation is required to be properly ingested by the various libraries. Spark ML has convenience functions for this purpose. We will need to prepare the CNC mill data to have features and labels: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To generate labels column from categorical data\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer\n",
    "\n",
    "# To generate features vector\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert class strings (categorical) to numerical label\n",
    "\n",
    "Can we predict the tool condition on a CNC mill, given a whole lot of sensor data? We have labels for `tool_condition`, namely 'worn' and 'unworn'. We need to convert these to a numerical format.\n",
    "\n",
    "**NOTE:** the off-the-shelf classifiers in Spark ML tend to have default names of the features ('features') and the class labels ('label'). We could alter these with `featuresCol='features'`, and `labelCol='label'`; we instead choose the default names hereafter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnc_class_labels = StringIndexer(inputCol='tool_condition', outputCol='label')\n",
    "cnc_data = cnc_class_labels.fit(cnc_data).transform(cnc_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the non-string features to a feature vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converted features and labels for CNC wear dataset:\n",
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(49,[0,3,6,7,8,11...|  1.0|\n",
      "|[148.0,2.0,700.0,...|  1.0|\n",
      "|[141.0,0.125,68.7...|  1.0|\n",
      "|[149.0,0.025,12.5...|  1.0|\n",
      "|[162.0,3.17,-12.5...|  0.0|\n",
      "|[145.0,-2.13,-25....|  0.0|\n",
      "|[142.0,-3.55,-25....|  0.0|\n",
      "|[167.0,-17.9,-12....|  0.0|\n",
      "|(49,[0,1,2,3,6,7,...|  0.0|\n",
      "|[150.0,-3.2,-43.8...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnc_feature_labels = cnc_data.columns[0:47] + cnc_data.columns[49:51]\n",
    "cnc_feature_vector = VectorAssembler(\n",
    "    inputCols=cnc_feature_labels,\n",
    "    outputCol=\"features\")  # Note the strict naming convention here\n",
    "\n",
    "cnc_data = cnc_feature_vector.transform(cnc_data)\n",
    "\n",
    "print(\"\\nConverted features and labels for CNC wear dataset:\")\n",
    "cnc_data.sample(0.001).select(\"features\", \"label\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into training and test data at 75%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rows of data for training: 18948, testing: 6338\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnc_train, cnc_test = cnc_data.randomSplit([0.75, 0.25], 42)\n",
    "\n",
    "print(f\"\\nRows of data for training: {cnc_train.count()}, testing: {cnc_test.count()}\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning with Spark ML: Classification\n",
    "\n",
    "Once the data are prepared, it is relatively simple to apply the various algorithms in the Spark ML eco-system. We can easily produce a logistic regression classifier. We can even throw in an elastic net!: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "logReg = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "logRegModel = logReg.fit(cnc_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coefficients and intercept are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: (49,[],[]); Intercept: -0.10946107408676813\n"
     ]
    }
   ],
   "source": [
    "print(f\"Coefficients: {logRegModel.coefficients}; Intercept: {logRegModel.intercept}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy: 50.0%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "validation = logRegModel.transform(cnc_test).select(\"rawPrediction\", \"label\")\n",
    "evaluator = BinaryClassificationEvaluator().setMetricName(\"areaUnderROC\")\n",
    "\n",
    "print(f\"\\nValidation accuracy: {evaluator.evaluate(validation)*100}%\\n\" )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning: Linear Regression\n",
    "\n",
    "If you've ever had to compute a linear regression problem by hand, you'd know it's like watching concrete dry. So why not use a concrete example to illustrate this? \n",
    "\n",
    "This concrete data-set has compressive strength (in MPa) -- a continuous outcome -- of various concrete mixtures given a range of controlled conditions. This is a potentially promising application of linear regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_data = spark.read.csv(\"../data/manufacturing/concrete/concrete.csv\",\n",
    "                              header=True,\n",
    "                              inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We rename the last column (compressive strength) to 'label':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_data = concrete_data.withColumnRenamed(concrete_data.columns[-1], \"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to make the rest into a feature vector, and the rest of the logic is similar to that of the logistic regression example above. Once the logic is clear, we may wish to formalize this as a _pipeline_. This will apply each of the data transforms and model fits into a single operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_feature_labels = concrete_data.columns[0:8]\n",
    "concrete_feature_vector = VectorAssembler(\n",
    "    inputCols=concrete_feature_labels,\n",
    "    outputCol=\"features\")  # Note the strict naming convention here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Produce a random split at 75%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rows of data for training: 755, testing: 275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "concrete_train, concrete_test = concrete_data.randomSplit([0.75, 0.25], 42)\n",
    "\n",
    "print(f\"\\nRows of data for training: {concrete_train.count()}, testing: {concrete_test.count()}\\n\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "linReg = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8, solver='normal') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the feature vector and fit the model (_i.e._ make it concrete) in a single pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import Pipeline\n",
    "\n",
    "lin_pipeline = Pipeline(stages=[concrete_feature_vector, linReg])\n",
    "\n",
    "linRegModel = lin_pipeline.fit(concrete_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the coefficients and intercept. Note our model is now part of a `Pipeline` object, so we have to access the particular stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.0936665960519165,0.06849622690485915,0.04590487140394453,-0.19188302288262057,0.35295136896926355,-0.00045596407641536747,-0.002703227695496029,0.10675020499988706]\n",
      "Intercept: 32.23753858511898\n"
     ]
    }
   ],
   "source": [
    "print(f\"Coefficients: {linRegModel.stages[-1].coefficients}\")\n",
    "print(f\"Intercept: {linRegModel.stages[-1].intercept}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summarize the model over the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numIterations: 11\n",
      "objectiveHistory: [0.5, 0.42945313726996776, 0.23856623783097589, 0.23521281729190635, 0.2374158720444947, 0.23130903077422738, 0.23078584407547983, 0.23016805290289782, 0.22962161678497606, 0.22927831795910597, 0.22888638669437644]\n",
      "+-------------------+\n",
      "|          residuals|\n",
      "+-------------------+\n",
      "|  -8.23426309945674|\n",
      "| -9.722840003308011|\n",
      "| -4.758138503307559|\n",
      "|  5.870555683694814|\n",
      "|  7.889798301701813|\n",
      "| -9.325139420882834|\n",
      "|-5.9427853408823825|\n",
      "|  4.073654158119989|\n",
      "|  6.130817956126993|\n",
      "|-11.669990650086342|\n",
      "+-------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "RMSE: 10.688765723925156\n",
      "r2: 0.5985691067028458\n"
     ]
    }
   ],
   "source": [
    "trainingSummary = linRegModel.stages[-1].summary\n",
    "print(f\"numIterations: {trainingSummary.totalIterations}\")\n",
    "print(f\"objectiveHistory: {trainingSummary.objectiveHistory}\")\n",
    "\n",
    "trainingSummary.residuals.show(10)\n",
    "print(f\"RMSE: {trainingSummary.rootMeanSquaredError}\")\n",
    "print(f\"r2: {trainingSummary.r2}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can use the `RegressionEvaluator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE: 9.764103640724203\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "concrete_test = concrete_feature_vector.transform(concrete_test)\n",
    "validation = linRegModel.stages[-1].transform(concrete_test).select(\"prediction\", \"label\")\n",
    "evaluator = RegressionEvaluator()\n",
    "\n",
    "print(f\"\\nRMSE: {evaluator.evaluate(validation)}\\n\" )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "We quite arbitrarily chose the hyperparameters `regParam` (or $\\alpha$) and `elasticNetParam` ($\\lambda$) to have the values 0.3 and 0.8 respectively. How do we know this model is optimal?\n",
    "\n",
    "There are a handful of viable approaches. Spark ML has a `tuning` library to help address this issue. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Validation Split\n",
    "One possibility is to split a single validation set at random from the training data and select the best model, from a given range of hyperparameters, according to some evaluation metric. This is the purpose of `TrainValidationSplit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we make use of `ParamGridBuilder` to build a hypergrid of hyperparameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(linReg.regParam, [0.1, 0.01, 5.0])\\\n",
    "    .addGrid(linReg.elasticNetParam, np.arange(0.0, 1.25, 0.25))\\\n",
    "    .addGrid(linReg.fitIntercept, [False, True])\\\n",
    "    .build() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will split off 25% of the training data to a single validation set. We'll make use of the `linReg` linear regressor and `RegressionEvaluator` we created above (which has an RMSE default output metric): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainValSplit = TrainValidationSplit(estimator=linReg,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=RegressionEvaluator(),\n",
    "                           trainRatio=0.75) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then fit the best model, according to the evaluator, by exploring the hyperparameter grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "concrete_train = concrete_feature_vector.transform(concrete_train)\n",
    "\n",
    "linReg_tvs = trainValSplit.fit(concrete_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the model fares on the test (hold-out) set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+\n",
      "|            features|             label|        prediction|\n",
      "+--------------------+------------------+------------------+\n",
      "|[102.0,153.0,0.0,...|       7.675936308| 11.68325013759485|\n",
      "|[102.0,153.0,0.0,...|17.275510655999998| 14.07207973074146|\n",
      "|[102.0,153.0,0.0,...|25.460969728000002| 21.12481472003145|\n",
      "|[122.6,183.9,0.0,...|10.354550567999999|14.334646590158162|\n",
      "|[132.0,206.5,160....|33.306517131999996| 35.86086711714507|\n",
      "|[135.0,105.0,193....|       21.91154728| 26.71282535898905|\n",
      "|[135.7,203.5,0.0,...|       18.19871902|23.629099932873856|\n",
      "|[139.6,209.4,0.0,...|       28.23748958| 23.83271116420894|\n",
      "|[139.6,209.4,0.0,...|39.358047983999995|30.885446153498933|\n",
      "|[143.6,0.0,174.9,...|       15.42357812|28.578869026974154|\n",
      "+--------------------+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linReg_tvs.transform(concrete_test)\\\n",
    "    .select(\"features\", \"label\", \"prediction\")\\\n",
    "    .show(10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default `RegressionEvaluator` requires a `prediction` and a `label` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE: 9.699798635299148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linReg_valid = linReg_tvs.transform(concrete_test).select(\"prediction\", \"label\")\n",
    "evaluator = RegressionEvaluator()\n",
    "\n",
    "print(f\"\\nRMSE: {evaluator.evaluate(linReg_valid)}\\n\" )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What coefficients were chosen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model coefficients: [0.11253401543048715,0.09303561902777983,0.07383230015452041,-0.15546075058628298,0.3557274976302689,0.010842697393762984,0.010752687216045095,0.11375379014983852]\n",
      "Model intercept: -4.723985511829938\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model coefficients: {linReg_tvs.bestModel.coefficients}\")\n",
    "print(f\"Model intercept: {linReg_tvs.bestModel.intercept}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accessing the actual hyperparameters is currently an arcane process -- we have to query the underlying Java object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen regularization parameter: 0.1\n",
      "Chosen elastic net parameter: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Chosen regularization parameter: {linReg_tvs.bestModel._java_obj.getRegParam()}\")\n",
    "print(f\"Chosen elastic net parameter: {linReg_tvs.bestModel._java_obj.getElasticNetParam()}\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-fold Cross-validation\n",
    "\n",
    "We can potentially improve upon a single validation split by selecting a model by sequentially 'folding' out a small number ($k$) of non-overlapping validation sets. Each fold is used once as a test set against the remainder of the training data. As with the single validation split approach above, the model hyperparamers are selected from a grid of values, according to an evaluation metric.\n",
    "\n",
    "This is straight-forward to apply using the tuning library `CrossValidator` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator\n",
    "\n",
    "crossValid = CrossValidator(estimator=linReg,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=RegressionEvaluator(),\n",
    "                          numFolds=5)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have set $k=5$ and recycled the `paramGrid`, estimator and evaluator from above. This may take a while to train, depending on the parameter grid size and number of folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "linReg_cv = crossValid.fit(concrete_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, assessing the performance of the model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------------+\n",
      "|            features|             label|        prediction|\n",
      "+--------------------+------------------+------------------+\n",
      "|[102.0,153.0,0.0,...|       7.675936308|11.562356043988999|\n",
      "|[102.0,153.0,0.0,...|17.275510655999998|13.972489803035376|\n",
      "|[102.0,153.0,0.0,...|25.460969728000002|21.088122805934212|\n",
      "|[122.6,183.9,0.0,...|10.354550567999999|14.157038475393575|\n",
      "|[132.0,206.5,160....|33.306517131999996|36.311394477308795|\n",
      "|[135.0,105.0,193....|       21.91154728| 26.85741387402061|\n",
      "|[135.7,203.5,0.0,...|       18.19871902|23.537248783181592|\n",
      "|[139.6,209.4,0.0,...|       28.23748958|23.668648687889412|\n",
      "|[139.6,209.4,0.0,...|39.358047983999995|30.784281690788248|\n",
      "|[143.6,0.0,174.9,...|       15.42357812|28.449240002845045|\n",
      "|[143.7,170.2,132....|29.870858223999996|31.128387776817224|\n",
      "|[144.0,0.0,175.0,...|       15.41668336|  28.5989466130831|\n",
      "|[144.0,136.0,106....|       26.14492992|28.589750306635384|\n",
      "|[145.0,0.0,179.0,...|       10.53519328| 17.61138369040956|\n",
      "|[145.9,230.5,0.0,...|      32.720462532| 24.33106264429129|\n",
      "|[147.8,175.1,0.0,...|      26.922658848|25.346214292888337|\n",
      "|[149.0,118.0,92.0...|       23.51802636|25.742679478398454|\n",
      "|[150.0,237.0,0.0,...|37.431652039999996| 33.23761951488335|\n",
      "|[150.9,0.0,183.9,...|15.569747031999999|26.307086793324714|\n",
      "|[151.6,0.0,111.9,...|      12.180972492|17.221922056091255|\n",
      "+--------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "linReg_cv.transform(concrete_test)\\\n",
    "    .select(\"features\", \"label\", \"prediction\")\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RMSE: 9.692384232692069\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validation = linReg_cv.transform(concrete_test).select(\"prediction\", \"label\")\n",
    "evaluator = RegressionEvaluator()\n",
    "\n",
    "print(f\"\\nRMSE: {evaluator.evaluate(validation)}\\n\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model coefficients: [0.11253401543048715,0.09303561902777983,0.07383230015452041,-0.15546075058628298,0.3557274976302689,0.010842697393762984,0.010752687216045095,0.11375379014983852]\n",
      "Model intercept: -4.723985511829938\n"
     ]
    }
   ],
   "source": [
    "print(f\"Model coefficients: {linReg_tvs.bestModel.coefficients}\")\n",
    "print(f\"Model intercept: {linReg_tvs.bestModel.intercept}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Param (regParam): 0.01\n",
      "Best Param (elasticNetParam): 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f'Best Param (regParam): {linReg_cv.bestModel._java_obj.getRegParam()}')\n",
    "print(f'Best Param (elasticNetParam): {linReg_cv.bestModel._java_obj.getElasticNetParam()}')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to note that the elastic net parameter, $\\lambda$ has a minimal value compared to the single validation split procedure. Performance, however, has only marginally improved. This is a tough set to perform a linear regression upon, and likely requires more feature engineering to improve further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Level: Deep Learning with Spark \n",
    "\n",
    "All the buzzwords!\n",
    "\n",
    "Spark ML has a Multilayer Perceptron (MLP) built-in function. This is a feed-forward artificial neural network, used in many classification problems. It is a very powerful classifier and is relatively simple to implement in Spark. However, non-trivial models require a lot of computational power to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct MLP architecture by specifying layers as a list:\n",
    "* The input layer must be the same as the number of features \n",
    "* The output layer must be the same as the number of classes (here: two)\n",
    "\n",
    "We choose here two hidden layers, each of width 128."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_architecture = [len(cnc_feature_labels), 128, 128, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_template = MultilayerPerceptronClassifier(\n",
    "    maxIter=100, \n",
    "    layers=mlp_architecture, \n",
    "    blockSize=128, \n",
    "    seed=13579)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model (this will take a while...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_model = mlp_template.fit(cnc_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate against the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation accuracy: 67.71074245241383%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "validation = mlp_model.transform(cnc_test).select(\"rawPrediction\", \"label\")\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "\n",
    "print(f\"\\nValidation accuracy: {evaluator.evaluate(validation)*100}%\\n\" )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PSA: Close your Spark Session!\n",
    "\n",
    "Don't forget to close your session!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This has given an overview of the Spark environment. We have covered:\n",
    " *  The conceptual model of Spark and the RDD distributed compute model\n",
    " *  The use of `SparkContext` and `SparkSession` to initiate Spark instances on a cluster\n",
    " *  Transformations, actions and where Spark performs lazy evaluation\n",
    " *  How to create, query and manipulate Spark SQL DataFrames\n",
    " *  How to persist RDD/DataFrame objects to improve performance\n",
    " *  Type conversion and filtering of Spark SQL DataFrames\n",
    " *  How to perform Exploratory Data Analysis (EDA) on large datasets using Spark\n",
    " *  How to create user-defined functions to customize work-flows\n",
    " *  How to perform machine learning, including classification, regression and deep learning using the Spark ML API\n",
    " \n",
    " Hopefully this gives you good fundamentals to explore the Spark framework further!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {
    "height": "552.4px",
    "left": "0px",
    "right": "833.6px",
    "top": "67.6px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
